{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T16:44:03.481568Z",
     "start_time": "2026-01-20T16:44:03.479024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.tree_util as jtu\n",
    "from jax import vmap, jit, Array\n",
    "import equinox as eqx\n",
    "from equinox import filter_jit"
   ],
   "id": "9187e754e7ddea90",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T16:44:03.711797Z",
     "start_time": "2026-01-20T16:44:03.710038Z"
    }
   },
   "cell_type": "code",
   "source": "from kernax import AbstractKernel, StaticAbstractKernel, WrapperKernel",
   "id": "1cc732598db6251a",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T16:44:04.035086Z",
     "start_time": "2026-01-20T16:44:04.029119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BlockKernel(WrapperKernel):\n",
    "\t\"\"\"\n",
    "\tWrapper kernel to build block covariance matrices using any kernel.\n",
    "\n",
    "\tA basic kernel usually works on inputs of shape (N, I), and produces covariance matrices of shape (N, N).\n",
    "\n",
    "\tWrapped inside a block kernel, they can either:\n",
    "\t- still work on inputs of shape (N, I), but produce covariance matrices of shape (B*N, B*N), where B is the number of blocks. This is useful when the hyperparameters are distinct to blocks, i.e. each sub-matrix has its own set of hyperparameters.\n",
    "\t- or work on inputs of shape (B, N, I), producing covariance matrices of shape (B*N, B*N). This is useful when inputs are different for each block, regardless of whether the hyperparameters are shared between blocks or not.\n",
    "\n",
    "\tThis class uses vmap to vectorize the kernel computation of each block, then resize the result into a block matrix.\n",
    "\t\"\"\"\n",
    "\n",
    "\tinner_kernel: AbstractKernel = eqx.field()\n",
    "\tnb_blocks: int = eqx.field(static=True)\n",
    "\tblock_in_axes: bool = eqx.field(static=True)\n",
    "\tblock_over_inputs: int | None = eqx.field(static=True)\n",
    "\n",
    "\tdef __init__(self, inner_kernel, nb_blocks, block_in_axes=None, block_over_inputs=True):\n",
    "\t\t\"\"\"\n",
    "\t\t:param inner_kernel: the kernel to wrap, must be an instance of AbstractKernel\n",
    "\t\t:param nb_blocks: the number of blocks\n",
    "\t\t:param block_in_axes: a pytree indicating which hyperparameters change across blocks.\n",
    "\t\t\t\t\t\t\t\tIf 0, the hyperparameter changes across the columns of the block matrix.\n",
    "\t\t\t\t\t\t\t\tIf 1, the hyperparameter changes across the rows of the block matrix.\n",
    "\t\t\t\t\t\t\t\tIf None, the hyperparameter is shared across all blocks.\n",
    "\t\t\t\t\t\t\t\tTo compute the block matrix, the kernel needs to have at least one of its hyperparameters changing across rows and one across columns.\n",
    "\t\t:param block_over_inputs: whether to expect inputs of shape (B, N, I) (True) or (N, I) (False)\n",
    "\n",
    "\t\tN.b: the result of this kernel is not always a valid covariance matrix! For example, an RBF kernel with a varying lengthscale across rows and a varying amplitude across column will not produce a symmetric matrix, hence giving an invalid covariance matrix.\n",
    "\t\tUsually, you want to use this kernel with an appropriate inner_kernel, calculating a function where two hyper-parameters have symmetric roles.\n",
    "\t\tA good example is a multi-output (convolutional) kernel in GPs, which usually have two distinct lengthscales (and variances) depending on which output dimension is considered.\n",
    "\t\t\"\"\"\n",
    "\t\t# Initialize the WrapperKernel\n",
    "\t\tsuper().__init__(inner_kernel=inner_kernel)\n",
    "\n",
    "\t\t# TODO: explicit error message when nb_blocks is 1, as vmap is not needed then\n",
    "\t\t# TODO: check that at least one hyperparameter varies across rows and one across columns\n",
    "\n",
    "\t\tself.nb_blocks = nb_blocks\n",
    "\n",
    "\t\t# Default: all array hyperparameters are shared (None for all array leaves)\n",
    "\t\tif block_in_axes is None:\n",
    "\t\t\t# Extract only array leaves and map them to None\n",
    "\t\t\tself.block_in_axes = jtu.tree_map(lambda _: None, inner_kernel)\n",
    "\t\telse:\n",
    "\t\t\tself.block_in_axes = block_in_axes\n",
    "\n",
    "\t\tself.block_over_inputs = 0 if block_over_inputs else None\n",
    "\n",
    "\t\t# Add batch dimension to parameters where batch_in_axes is 0\n",
    "\t\tself.inner_kernel = jtu.tree_map(\n",
    "\t\t\tlambda param, block_in_ax: (\n",
    "\t\t\t\tparam if block_in_ax is None else jnp.repeat(param[None, ...], nb_blocks, axis=0)\n",
    "\t\t\t),\n",
    "\t\t\tself.inner_kernel,\n",
    "\t\t\tself.block_in_axes,\n",
    "\t\t)\n",
    "\n",
    "\t@filter_jit\n",
    "\tdef __call__(self, x1: jnp.ndarray, x2: None | jnp.ndarray = None) -> jnp.ndarray:\n",
    "\t\t\"\"\"\n",
    "\t\tCompute the kernel over batched inputs using vmap.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\t\tx1: Input of shape (B, ..., N, I)\n",
    "\t\t\t\tx2: Optional second input of shape (B, ..., M, I)\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\t\tKernel block-matrix of appropriate shape\n",
    "\t\t\"\"\"\n",
    "\t\tx2 = x1 if x2 is None else x2\n",
    "\n",
    "\t\trows, cols = jnp.triu_indices(self.nb_blocks)\n",
    "\n",
    "\t\tfull_kernel = jtu.tree_map(\n",
    "\t\t\tlambda param, block_in_ax:\n",
    "\t\t\t\tparam[rows] if block_in_ax == 0 else param[cols] if block_in_ax == 1 else param,\n",
    "\t\t\tself.inner_kernel,\n",
    "\t\t\tself.block_in_axes,\n",
    "\t\t)\n",
    "\n",
    "\t\tx1 = x1[rows] if self.block_over_inputs == 0 else x1\n",
    "\t\tx2 = x2[cols] if self.block_over_inputs == 0 else x2\n",
    "\n",
    "\t\t# vmap over the batch dimension of inner_kernel and inputs\n",
    "\t\t# Each batch element gets its own version of inner_kernel with corresponding hyperparameters\n",
    "\t\treturn vmap(\n",
    "\t\t\tlambda kernel, x1, x2: kernel(x1, x2),\n",
    "\t\t\tin_axes=(\n",
    "\t\t\t\tjtu.tree_map(lambda x: None if x is None else 0, self.block_in_axes),\n",
    "\t\t\t\tself.block_over_inputs,\n",
    "\t\t\t\tself.block_over_inputs,\n",
    "\t\t\t),\n",
    "\t\t)(full_kernel, x1, x2)\n",
    "\n",
    "\tdef __str__(self):\n",
    "\t\treturn f\"Block{self.inner_kernel}\""
   ],
   "id": "4dce69d4f0777627",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T16:44:14.595540Z",
     "start_time": "2026-01-20T16:44:14.591809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class StaticMOKernel(StaticAbstractKernel):\n",
    "\t@classmethod\n",
    "\t@filter_jit\n",
    "\tdef pairwise_cov(cls, kern: AbstractKernel, x1: jnp.ndarray, x2: jnp.ndarray) -> jnp.ndarray:\n",
    "\t\t\"\"\"\n",
    "\t\tCompute the kernel covariance value between two vectors.\n",
    "\n",
    "\t\t:param kern: kernel instance containing the hyperparameters\n",
    "\t\t:param x1: scalar array\n",
    "\t\t:param x2: scalar array\n",
    "\t\t:return: scalar array\n",
    "\t\t\"\"\"\n",
    "\t\tkern = eqx.combine(kern)\n",
    "\n",
    "\t\t# As the formula only involves diagonal matrices, we can compute directly with vectors\n",
    "\t\tsigma_diag = jnp.exp(kern.length_scale_1) + jnp.exp(kern.length_scale_2) + jnp.exp(kern.length_scale_u)  # Σ\n",
    "\t\tsigma_det = jnp.prod(sigma_diag)  # |Σ|\n",
    "\t\tdiff = x1 - x2  # x - x'\n",
    "\n",
    "\t\t# Compute the quadratic form: (x - x')^T Sigma^{-1} (x - x')\n",
    "\t\t# Since Sigma^{-1} is diagonal, this simplifies to sum of (diff_i^2 * sigma_inv_diag_i)\n",
    "\t\tquadratic_form = jnp.sum(diff**2 / sigma_diag)\n",
    "\n",
    "\t\treturn jnp.exp(kern.variance_1) * jnp.exp(kern.variance_2) /(((2 * jnp.pi)**(len(x1)/2)) * jnp.sqrt(sigma_det)) * jnp.exp(-0.5 * quadratic_form)\n",
    "\n",
    "\n",
    "class MOKernel(AbstractKernel):\n",
    "\t\"\"\"\n",
    "\tSquared Exponential (aka \"RBF\" or \"Gaussian\") Kernel\n",
    "\t\"\"\"\n",
    "\n",
    "\tlength_scale_1: Array = eqx.field(converter=jnp.asarray)\n",
    "\tlength_scale_2: Array = eqx.field(converter=jnp.asarray)\n",
    "\tlength_scale_u: Array = eqx.field(converter=jnp.asarray)\n",
    "\tvariance_1: Array = eqx.field(converter=jnp.asarray)\n",
    "\tvariance_2: Array = eqx.field(converter=jnp.asarray)\n",
    "\n",
    "\tstatic_class = StaticMOKernel\n",
    "\n",
    "\tdef __init__(self, length_scale_1, length_scale_2, length_scale_u, variance_1, variance_2):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.length_scale_1 = length_scale_1\n",
    "\t\tself.length_scale_2 = length_scale_2\n",
    "\t\tself.length_scale_u = length_scale_u\n",
    "\t\tself.variance_1 = variance_1\n",
    "\t\tself.variance_2 = variance_2"
   ],
   "id": "d21be0141588b5a1",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T15:49:41.096402Z",
     "start_time": "2026-01-20T15:49:41.020582Z"
    }
   },
   "cell_type": "code",
   "source": "jnp.array([1, 2, 3])[jnp.array([0, 0, 1, 1, 0, 2])]",
   "id": "5afa1cb411f58b70",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([1, 1, 2, 2, 1, 3], dtype=int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "571ed97ff2c77fb7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
